# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12dZb0e4RnGaPIQ-BxQ0LLVcA2a7DUupl
"""

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file 1/0 (e.g. pd.read_csv)
import os  # to use operating system dependent functionality
import librosa  # to extract speech features
import wave  # read and write WAV files
import matplotlib.pyplot as plt  # to generate the visualizations

# MLP Classifier
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# LSTM Classifier
import keras
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import *
from keras.optimizers import RMSprop

print("loaded libraries")

from google.colab import drive
drive.mount('/content/drive')

def extract_mfcc(wav_file_name):
    """
    This function extracts MFCC features and obtains the mean of each dimension.

    Input:
    - wav_file_name: path to WAV file

    Output:
    - mfcc features
    """
    y, sr = librosa.load(wav_file_name)
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
    return mfccs

# Load radvess speech data
radvess_speech_labels = []  # to save extracted label/file
ravdess_speech_data = []  # to save extracted features/file

# Ensure that the specified directory exists
directory_path = '/content/drive/MyDrive/Colab Notebooks/voiceDataset'
if os.path.exists(directory_path):
    for dirname, _, filenames in os.walk(directory_path):
        for filename in filenames:
            # print(os.path.join(dirname, filename))
            radvess_speech_labels.append(int(filename[7]))  # Adjusted the index to 7 only
            wav_file_name = os.path.join(dirname, filename)
            ravdess_speech_data.append(extract_mfcc(wav_file_name))  # extract MFCC features/file

    print("Finish Loading the Dataset")
else:
    print("Directory not found:", directory_path)

ravdess_speech_data

#### convert data and label to array
ravdess_speech_data_array = np.asarray(ravdess_speech_data)  # convert the input to an array
ravdess_speech_label_array = np.array(radvess_speech_labels)
ravdess_speech_label_array.shape # get tuple of array dimensions
#### make categorical labels
labels_categorical = to_categorical(ravdess_speech_label_array)  # converts a class vector (integers) to binary class matrix
labels_categorical.shape

ravdess_speech_data_array

x_train, x_test, y_train, y_test = train_test_split(np.array(ravdess_speech_data_array),
                                                    labels_categorical, test_size=0.20, random_state=9)

# Split the training, validating, and testing sets
number_of_samples = ravdess_speech_data_array.shape[0]
training_samples = int(number_of_samples * 0.8)
validation_samples = int (number_of_samples * 0.1)
test_samples = int (number_of_samples * 0.1)

# Define the LSTM model
def create_model_LSTM():
    model = Sequential()
    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))
    model.add(Dense(64))
    model.add(Dropout(0.4))
    model.add(Activation('relu'))
    model.add(Dense(32))
    model.add(Dropout(0.4))
    model.add(Activation('relu'))
    model.add(Dense(9))  # Adjust the number of units to match the number of classes in your dataset
    model.add(Activation('softmax'))
    # Configures the model for training
    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
    return model

w = np. expand_dims(ravdess_speech_data_array[:training_samples],-1)

w.shape

model_A = create_model_LSTM()  # Create the LSTM model

# Define the number of training and validation samples
training_samples = int(0.8 * ravdess_speech_data_array.shape[0])  # 80% of the data for training
validation_samples = ravdess_speech_data_array.shape[0] - training_samples

history = model_A.fit(
    np.expand_dims(ravdess_speech_data_array[:training_samples], -1),  # Slice input data for training
    labels_categorical[:training_samples],  # Slice labels for training
    validation_data=(
        np.expand_dims(ravdess_speech_data_array[training_samples:training_samples+validation_samples], -1),# Slice input data for validation
        labels_categorical[training_samples:training_samples+validation_samples]  # Slice labels for validation
    ),
    epochs=134,
    shuffle=True
)

loss = history.history['loss']
val_loss = history.history['val_loss']

# Creating epochs range
epochs = range(1, len(loss) + 1)

# Plotting the loss
plt.plot(epochs, loss, 'ro', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')

plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.legend()

plt.show()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']


# Plotting the accuracy
plt.plot(epochs, acc, 'ro', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')

plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

plt.legend()

plt.show()

total_samples = ravdess_speech_data_array.shape[0]
print("Total number of samples:", total_samples)

training_ratio = 0.8  # 80% of the data for training
validation_ratio = 0.1  # 10% of the data for validation
testing_ratio = 0.1  # 10% of the data for testing

training_samples = int(training_ratio * total_samples)
validation_samples = int(validation_ratio * total_samples)
testing_samples = total_samples - training_samples - validation_samples

print("Training samples:", training_samples)
print("Validation samples:", validation_samples)
print("Testing samples:", testing_samples)

if (training_samples + validation_samples + testing_samples) == total_samples:
    print("Indices are within the bounds of the dataset.")
else:
    print("Indices are not within the bounds of the dataset.")

print("Test data shape:", ravdess_speech_data_array[training_samples + validation_samples:].shape)
print("Test labels shape:", labels_categorical[training_samples + validation_samples:].shape)
print("Training samples:", training_samples)
print("Validation samples:", validation_samples)

# Check the shape of the input data
print("Shape of input data:", np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], 1).shape)

# Check the expected input shape of the model
print("Expected input shape of the model:", model_A.input_shape)

import numpy as np

# Assuming ravdess_speech_data_array contains your input data
# Shape of input data: (288, 1, 40)
input_data = ravdess_speech_data_array[training_samples + validation_samples:]

# Reshape the input data
reshaped_input_data = np.reshape(input_data, (input_data.shape[0], 40, 1))

# Check the shape of the reshaped input data
print("Shape of reshaped input data:", reshaped_input_data.shape)

test_loss, test_accuracy = model_A.evaluate(
    np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], axis=-1),
    labels_categorical[training_samples + validation_samples:]
)

import numpy as np
import tensorflow as tf
from keras.models import Model

# Review Model Architecture
print("Model Summary:")
print(model_A.summary())

# Check Expected Input Shape
print("Expected input shape of the model:", model_A.input_shape)

# Check the Shape of Input Data
print("Shape of input data:", np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], 1).shape)

try:
    # Enable eager execution
    tf.config.run_functions_eagerly(True)

    # Perform a forward pass through the model
    input_data = np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], 1)
    output = model_A(input_data)
    print("Output Shape:", output.shape)

    # Disable eager execution (optional)
    tf.config.run_functions_eagerly(False)

except Exception as e:
    print("Error Message:", e)

# Perform a forward pass through the model
predictions = model_A.predict(np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], axis=2))

emotions = {1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad', 5: 'angry', 6: 'fearful', 7: 'disgust', 8: 'surprised'}

def predict(wav_filepath):
    # Extract MFCC features from the audio file
    test_point = extract_mfcc(wav_filepath)

    # Reshape the test_point to match the input shape of the model
    test_point = np.reshape(test_point, newshape=(1, 40, 1))

    # Make predictions using the model
    predictions = model_A.predict(test_point)
    print(emotions[np.argmax(predictions[0]) + 1])

predict('/content/drive/MyDrive/Colab Notebooks/voiceDataset/Actor_01/03-01-01-01-01-01-01.wav')

predict('/content/drive/MyDrive/Colab Notebooks/voiceDataset/Actor_03/03-01-06-02-02-02-03.wav')

predict('/content/drive/MyDrive/Colab Notebooks/voiceDataset/Actor_22/03-01-02-01-01-01-22.wav')

model_A.save('/content/drive/MyDrive/Colab_Notebooks/mymodel.h5')

predict('/content/drive/MyDrive/Colab Notebooks/voiceDataset/Actor_11/03-01-03-02-01-01-11.wav')

predict('/content/drive/MyDrive/Colab Notebooks/voiceDataset/Actor_04/03-01-02-01-02-01-04.wav')

predict('/content/drive/MyDrive/Colab Notebooks/voiceDataset/Actor_04/03-01-02-01-02-01-04.wav')